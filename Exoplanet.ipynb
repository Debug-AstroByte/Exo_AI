{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd4de0-6523-4f51-95c6-ad753a1b64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy pandas matplotlib seaborn scikit-learn joblib astropy scipy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6e863-d198-4984-acba-7878bbad2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow lightkurve shap streamlit --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e69377-3ea2-4e95-8a48-55015f2a0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "from lightkurve import search_lightcurvefile\n",
    "from lightkurve import search_lightcurve\n",
    "from lightkurve import LightCurveFile \n",
    "from astropy.stats import sigma_clip\n",
    "from scipy.signal import convolve\n",
    "import shap\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f188b5a-6aac-4134-bfae-9d840c4d2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file name\n",
    "FILE_NAME = \"kepler_koi_clean.csv\"\n",
    "df = None # Initialize DataFrame\n",
    "\n",
    "# --- 1. Check if the file already exists locally ---\n",
    "if os.path.exists(FILE_NAME):\n",
    "    print(f\"Loading KOI data from existing file: {FILE_NAME}\")\n",
    "    try:\n",
    "        # Load the local CSV file\n",
    "        df = pd.read_csv(FILE_NAME)\n",
    "        print(\"Data loaded successfully from local drive.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading local file: {e}. Attempting to re-download...\")\n",
    "        df = None # Force re-download if the file is corrupted\n",
    "\n",
    "# --- 2. If file is missing or corrupted, fetch from NASA ---\n",
    "if df is None:\n",
    "    print(\"Local file not found or corrupted. Fetching Kepler KOI data from NASA (5+ minutes)...\")\n",
    "    try:\n",
    "        # Correct way to fetch KOI table\n",
    "        koi_table = NasaExoplanetArchive.query_criteria(\n",
    "            table=\"cumulative\",\n",
    "            select=\"*\" # Fetch all columns\n",
    "        )\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        df = koi_table.to_pandas()\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(FILE_NAME, index=False)\n",
    "\n",
    "        print(f\"Data successfully fetched and saved as {FILE_NAME}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching KOI data from NASA:\", e)\n",
    "        df = None # Final failure\n",
    "\n",
    "# --- 3. Final Output and Checks ---\n",
    "if df is not None:\n",
    "    print(f\"\\nFinal Dataset Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nValue counts for disposition:\")\n",
    "    print(df['koi_disposition'].value_counts())\n",
    "else:\n",
    "    print(\"\\nFATAL ERROR: Could not load data from local file or NASA server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409554ca-1f16-480e-b44f-786f56ca9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for Large-Scale Real Kepler Pipeline\n",
    "CACHE_DIR = './kepler_cache' \n",
    "\n",
    "# Set lightkurve environment variable to force it to IGNORE its default, corrupted cache\n",
    "os.environ['LIGHTKURVE_CACHE_DIR'] = CACHE_DIR\n",
    "\n",
    "# CRITICAL: Clean and recreate the directory to remove ALL corrupted files\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "print(f\"Set LIGHTKURVE_CACHE_DIR to: {CACHE_DIR}\")\n",
    "print(\"Cache directory has been cleaned and reset.\")\n",
    "\n",
    "N_SAMPLES_PER_CLASS = 200\n",
    "N_BINS = 400\n",
    "MAX_LEN = N_BINS\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Section 8: Execute External Parallel Processing Script\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# CRITICAL: This runs the process_data.py script outside the unstable kernel environment\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(\"Starting external script. This should now run without I/O errors.\")\n",
    "print(\"Check the VS Code Terminal/Output tab for real-time progress.\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "# Execute the external Python file\n",
    "!python process_data.py\n",
    "\n",
    "# Load the saved results back into the notebook environment\n",
    "try:\n",
    "    X_arr, y_arr = joblib.load('processed_data_output.pkl')\n",
    "    \n",
    "    print(\"\\n--- Processed Data Successfully Loaded Back into Notebook ---\")\n",
    "    print(f\"Final loaded data shape: {X_arr.shape}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_arr)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nERROR: Could not find 'processed_data_output.pkl'.\")\n",
    "    print(\"The script likely failed. Check the 'parallel_processing_script.log' file for the error.\")\n",
    "    \n",
    "# Print log summary for debugging\n",
    "print(\"\\n--- LOG FILE SUMMARY (for debugging) ---\")\n",
    "try:\n",
    "    with open('parallel_processing_script.log', 'r') as f:\n",
    "        log_lines = f.readlines()\n",
    "        for line in log_lines[-10:]:\n",
    "            print(line.strip())\n",
    "except FileNotFoundError:\n",
    "    print(\"Log file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbaa55-64ef-40e8-ac75-d28778ecd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split and Class Weights\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, test_size = 42, stratify = y_arr, random_state = 42)\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes = classes, y = y_train)\n",
    "class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "print('Class Weights:', class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1195edc-eb67-4e31-92a6-f4e4c138e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D CNN Definition\n",
    "def build_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(32, 9, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "model = build_cnn(X_train.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda33078-fd0f-4a1a-88ce-0210a02a254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with #Callbacks\n",
    "cb = [callbacks.EarlyStopping(monitor = 'val_auc', patience = 6, restore_best_weights = True, mode = 'max'), \n",
    "      callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 3, min_lr = 1e-6, verbose = 1), \n",
    "      callbacks.ModelCheckpoint('best_cnn_kepler.h5', monitor  = 'val_auc', save_best_only = True, mode = 'max')]\n",
    "history = model.fit(X_train, y_train, validation_split=0.15, epochs=EPOCHS, batch_size=BATCH_SIZE, class_weight=class_weight_dict, callbacks=cb, verbose=2)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4812a38-776a-47d5-98bc-dea6cf84c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation, ROC, PR, and Confusion Matrix\n",
    "y_pred = model.predict(X_test).ravel()  \n",
    "\n",
    "#ROC CURVE \n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#Precision Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC={roc_auc:.3f}')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(recall, precision, label=f'PR AUC={pr_auc:.3f}')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, (y_pred>0.5).astype(int))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['FalsePositive','Confirmed'])\n",
    "disp.plot(cmap='Blues'); plt.show()\n",
    "\n",
    "# Visualize top predicted planets\n",
    "for i in np.argsort(y_pred)[-6:][::-1]:\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(X_test[i].squeeze(), label=f'pred={y_pred[i]:.3f}, label={y_test[i]}')\n",
    "    plt.title('Folded / Resampled Light Curve')\n",
    "    plt.xlabel('Phase bin'); plt.legend(); plt.show()\n",
    "    \n",
    "# Save preprocessed arrays\n",
    "np.savez_compressed('kepler_200_dataset.npz', X_train=X_train, X_test=X_test,\n",
    "y_train=y_train, y_test=y_test)\n",
    "model.save('cnn_kepler_200_v2.h5')\n",
    "print('Saved dataset and CNN model.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
